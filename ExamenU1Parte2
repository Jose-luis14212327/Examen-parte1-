println("Examen Unidad 1 / Parte 2 Scala Dataframes")

//Ejercicio 1
cd spark-2.3.1-bin-hadoop2.7/
cd bin
./spark-shell


//Ejercicio 2
import org.apache.spark.sql.SparkSession

val spar = SparkSession.builder().getOrCreate()

val df = spark.read.option("header", "true").option("inferSchema","true")csv("Netflix_2011_2016.csv")

//Ejercicio 3
df.columns

//Ejercicio 4
df.printSchema()

//Ejercicio 5
df.select($"Date", $"Open",$"High",$"Low",$"Close").show()

//Ejercicio 6
df.describe().show()

//Ejercicio 7
val df2 = df.withColumn("HV Ratio", df("High")/ df("Volume"))
df2.printSchema()
df2.show()

//Ejercicio 8
df.orderBy($"High".desc).show(1)

//Ejercicio 9
println("Son los valores con las que cerro la bolsa de valores de Netflix")

//Ejercicio 10
df.select(min(df("Volume"))).show
df.select(max(df("Volume"))).show


//Ejercicio 11

(a)
df.filter($"Close"<600).count()


(b)
(df.filter($"High" > 500).count() * 1.0/ df.count())*100

(c)
df.select(corr("High","Volume")).show()

(d)
val yeardf = df.withColumn("Year",year(df("Date")))
val yearmaxs = yeardf.select($"Year",$"High").groupBy("Year").max()
val res = yearmaxs.select($"Year",$"max(High)")
res.show()

(e)
val monthdf = df.withColumn("Month",month(df("Date")))
val monthavgs = monthdf.select($"Month",$"Close").groupBy("Month").mean()
monthavgs.select($"Month",$"avg(Close)").orderBy("Month").show()
